import pandas as pd
import numpy as np
import os
import uuid
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score, confusion_matrix, classification_report

# –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤, —è–∫—â–æ –≤–æ–Ω–∞ —â–µ –Ω–µ —ñ—Å–Ω—É—î
PLOT_DIR = "static/predict_plots"
os.makedirs(PLOT_DIR, exist_ok=True)

# –§—É–Ω–∫—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞ —É —Ñ–∞–π–ª —ñ–∑ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º —ñ–º–µ–Ω–µ–º
def save_plot():
    plot_id = f"{uuid.uuid4().hex}.png"
    path = os.path.join(PLOT_DIR, plot_id)
    plt.tight_layout()
    plt.savefig(path, bbox_inches='tight')
    plt.close()
    return f"/{path}"

# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É –º–æ–¥–µ–ª—ñ (–ª—ñ–Ω—ñ–π–Ω–æ—ó –∞–±–æ –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó)
def run_model(model_type, data, target, features):
    df = pd.DataFrame(data)

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ—Ä–µ–∫—Ç–Ω–æ—Å—Ç—ñ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
    if not target or not isinstance(target, str) or target not in df.columns:
        return "<p>‚ùå Target column is invalid or not selected.</p>"

    # –í—ñ–¥–±–∏—Ä–∞—î–º–æ –ª–∏—à–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
    df = df[features + [target]].copy()
    df = df.dropna()

    # –ü—Ä–∏–≤–æ–¥–∏–º–æ –≤—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ —Ç–∏–ø—É
    X = df[features].apply(pd.to_numeric, errors='coerce')
    y = pd.to_numeric(df[target], errors='coerce')
    df = pd.concat([X, y], axis=1).dropna()

    removed = []  # –û–∑–Ω–∞–∫–∏, —è–∫—ñ –±—É–¥—É—Ç—å –≤–∏–¥–∞–ª–µ–Ω—ñ
    kept = []     # –û–∑–Ω–∞–∫–∏, —è–∫—ñ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è

    # –í–∏–¥–∞–ª—è—î–º–æ –æ–∑–Ω–∞–∫–∏ –∑ –º–∞–ª–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é –∞–±–æ –ø–æ—Å—Ç—ñ–π–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º
    for col in features:
        if df[col].nunique() <= 1:
            removed.append(col)
            continue
        corr = np.corrcoef(df[col], y)[0, 1]
        if abs(corr) < 0.1:
            removed.append(col)
        else:
            kept.append(col)

    if not kept:
        return "<p>üö´ No features with sufficient correlation to target.</p>"

    X = df[kept]
    y = df[target]

    # –†–æ–∑–±–∏–≤–∞—î–º–æ –¥–∞–Ω—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π —ñ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±–æ—Ä–∏
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

    messages = ""
    if removed:
        messages += "<p>‚ö†Ô∏è The following features were removed due to low correlation: <b>" + ", ".join(removed) + "</b></p>"

    # –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
    if model_type == "linear":
        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)

        # –¢–µ–∫—Å—Ç–æ–≤–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        if r2 < 0.3:
            interp = "Model explains very little of the target variance."
        elif r2 < 0.6:
            interp = "Model has moderate explanatory power."
        else:
            interp = "Model fits the data well."

        # –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ —Ä–µ–∞–ª—å–Ω—ñ vs –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        fig, ax = plt.subplots()
        ax.scatter(y_test, y_pred, alpha=0.6)
        ax.set_xlabel("Actual")
        ax.set_ylabel("Predicted")
        ax.set_title("Prediction vs Actual")
        plot_url = save_plot()

        return f"""
        {messages}
        <h3>üìò Linear Regression</h3>
        <p><b>R¬≤ Score:</b> {r2:.3f}</p>
        <p><b>Mean Absolute Error:</b> {mae:.3f}</p>
        <p>{interp}</p>
        <img src="{plot_url}">
        """

    elif model_type == "logistic":
        model = LogisticRegression(max_iter=1000)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        acc = accuracy_score(y_test, y_pred)

        if acc < 0.6:
            interp = "Low predictive quality."
        elif acc < 0.8:
            interp = "Acceptable accuracy."
        else:
            interp = "High accuracy ‚Äî model performs well."

        cm = confusion_matrix(y_test, y_pred)

        # –ü–æ–±—É–¥–æ–≤–∞ –º–∞—Ç—Ä–∏—Ü—ñ –ø–ª—É—Ç–∞–Ω–∏–Ω–∏
        fig, ax = plt.subplots()
        ax.matshow(cm, cmap="Blues")
        ax.set_xlabel("Predicted")
        ax.set_ylabel("Actual")
        ax.set_title("Confusion Matrix")
        plot_url = save_plot()

        report = classification_report(y_test, y_pred)

        return f"""
        {messages}
        <h3>üìó Logistic Regression</h3>
        <p><b>Accuracy:</b> {acc:.3f}</p>
        <p>{interp}</p>
        <pre>{report}</pre>
        <img src="{plot_url}">
        """
